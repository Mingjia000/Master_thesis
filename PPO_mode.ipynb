{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF/Gw7njOcuz09Za0Fzgax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mingjia000/Master_thesis/blob/main/PPO_mode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "QglHwkhg4m7u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhZJGrw00R_j",
        "outputId": "d7af7ab9-ec3a-4125-dd19-a596d2309552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Device set to : cpu\n",
            "============================================================================================\n",
            "training environment name : short path\n",
            "current logging run number for short path :  7\n",
            "logging at : PPO_logs/short path//PPO_short path_log_7.csv\n",
            "save checkpoint path : PPO_preTrained/short path/PPO_short path_0_0.pth\n",
            "Started training at (GMT) :  2022-09-22 17:10:34\n",
            "============================================================================================\n",
            "Episode : 318 \t\t Timestep : 1600 \t\t Average Reward : 204.89\n",
            "Episode : 733 \t\t Timestep : 3200 \t\t Average Reward : 468.43\n",
            "Episode : 1254 \t\t Timestep : 4800 \t\t Average Reward : 571.51\n",
            "Episode : 1892 \t\t Timestep : 6400 \t\t Average Reward : 668.03\n",
            "Episode : 2598 \t\t Timestep : 8000 \t\t Average Reward : 725.65\n",
            "Episode : 3305 \t\t Timestep : 9600 \t\t Average Reward : 764.4\n",
            "Episode : 4032 \t\t Timestep : 11200 \t\t Average Reward : 798.66\n",
            "Episode : 4758 \t\t Timestep : 12800 \t\t Average Reward : 821.06\n",
            "Episode : 5503 \t\t Timestep : 14400 \t\t Average Reward : 841.09\n",
            "Episode : 6253 \t\t Timestep : 16000 \t\t Average Reward : 866.38\n",
            "Episode : 7014 \t\t Timestep : 17600 \t\t Average Reward : 883.0\n",
            "Episode : 7775 \t\t Timestep : 19200 \t\t Average Reward : 891.11\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/short path/PPO_short path_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:13\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 8536 \t\t Timestep : 20800 \t\t Average Reward : 897.36\n",
            "Episode : 9283 \t\t Timestep : 22400 \t\t Average Reward : 898.56\n",
            "Episode : 10015 \t\t Timestep : 24000 \t\t Average Reward : 904.11\n",
            "Episode : 10745 \t\t Timestep : 25600 \t\t Average Reward : 909.31\n",
            "Episode : 11432 \t\t Timestep : 27200 \t\t Average Reward : 920.84\n",
            "Episode : 11958 \t\t Timestep : 28800 \t\t Average Reward : 899.57\n",
            "Episode : 12577 \t\t Timestep : 30400 \t\t Average Reward : 950.64\n",
            "Episode : 13148 \t\t Timestep : 32000 \t\t Average Reward : 973.08\n",
            "Episode : 13694 \t\t Timestep : 33600 \t\t Average Reward : 983.96\n",
            "Episode : 14238 \t\t Timestep : 35200 \t\t Average Reward : 988.99\n",
            "Episode : 14771 \t\t Timestep : 36800 \t\t Average Reward : 989.69\n",
            "Episode : 15305 \t\t Timestep : 38400 \t\t Average Reward : 991.94\n",
            "Episode : 15840 \t\t Timestep : 40000 \t\t Average Reward : 992.87\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/short path/PPO_short path_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:27\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 16376 \t\t Timestep : 41600 \t\t Average Reward : 994.55\n",
            "Episode : 16904 \t\t Timestep : 43200 \t\t Average Reward : 990.72\n",
            "Episode : 17437 \t\t Timestep : 44800 \t\t Average Reward : 993.41\n",
            "Episode : 17970 \t\t Timestep : 46400 \t\t Average Reward : 994.34\n",
            "Episode : 18504 \t\t Timestep : 48000 \t\t Average Reward : 995.08\n",
            "Episode : 19037 \t\t Timestep : 49600 \t\t Average Reward : 994.14\n",
            "Episode : 19570 \t\t Timestep : 51200 \t\t Average Reward : 995.26\n",
            "Episode : 20104 \t\t Timestep : 52800 \t\t Average Reward : 995.45\n",
            "Episode : 20638 \t\t Timestep : 54400 \t\t Average Reward : 995.63\n",
            "Episode : 21170 \t\t Timestep : 56000 \t\t Average Reward : 994.69\n",
            "Episode : 21703 \t\t Timestep : 57600 \t\t Average Reward : 995.82\n",
            "Episode : 22237 \t\t Timestep : 59200 \t\t Average Reward : 994.52\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/short path/PPO_short path_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:41\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 22769 \t\t Timestep : 60800 \t\t Average Reward : 995.62\n",
            "Episode : 23303 \t\t Timestep : 62400 \t\t Average Reward : 996.0\n",
            "Episode : 23836 \t\t Timestep : 64000 \t\t Average Reward : 995.81\n",
            "Episode : 24370 \t\t Timestep : 65600 \t\t Average Reward : 995.63\n",
            "Episode : 24903 \t\t Timestep : 67200 \t\t Average Reward : 995.44\n",
            "Episode : 25436 \t\t Timestep : 68800 \t\t Average Reward : 995.63\n",
            "Episode : 25970 \t\t Timestep : 70400 \t\t Average Reward : 996.0\n",
            "Episode : 26503 \t\t Timestep : 72000 \t\t Average Reward : 996.0\n",
            "Episode : 27036 \t\t Timestep : 73600 \t\t Average Reward : 995.81\n",
            "Episode : 27570 \t\t Timestep : 75200 \t\t Average Reward : 995.63\n",
            "Episode : 28103 \t\t Timestep : 76800 \t\t Average Reward : 995.81\n",
            "Episode : 28637 \t\t Timestep : 78400 \t\t Average Reward : 995.63\n",
            "Episode : 29170 \t\t Timestep : 80000 \t\t Average Reward : 995.81\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/short path/PPO_short path_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:56\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 29703 \t\t Timestep : 81600 \t\t Average Reward : 995.81\n",
            "Episode : 30237 \t\t Timestep : 83200 \t\t Average Reward : 995.63\n",
            "Episode : 30770 \t\t Timestep : 84800 \t\t Average Reward : 996.0\n",
            "Episode : 31304 \t\t Timestep : 86400 \t\t Average Reward : 995.82\n",
            "Episode : 31837 \t\t Timestep : 88000 \t\t Average Reward : 996.0\n",
            "Episode : 32370 \t\t Timestep : 89600 \t\t Average Reward : 995.81\n",
            "Episode : 32903 \t\t Timestep : 91200 \t\t Average Reward : 995.62\n",
            "Episode : 33437 \t\t Timestep : 92800 \t\t Average Reward : 995.63\n",
            "Episode : 33970 \t\t Timestep : 94400 \t\t Average Reward : 996.0\n",
            "Episode : 34504 \t\t Timestep : 96000 \t\t Average Reward : 995.82\n",
            "Episode : 35037 \t\t Timestep : 97600 \t\t Average Reward : 996.0\n",
            "Episode : 35570 \t\t Timestep : 99200 \t\t Average Reward : 996.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/short path/PPO_short path_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:09\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2022-09-22 17:10:34\n",
            "Finished training at (GMT) :  2022-09-22 17:11:43\n",
            "Total training time  :  0:01:09\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "################################## set device ##################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# set device to cpu or cuda\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()): \n",
        "    device = torch.device('cuda:0') \n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")\n",
        "    \n",
        "print(\"============================================================================================\")\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Softmax(dim=-1)\n",
        "                        )\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Linear(64, 1)\n",
        "                    )\n",
        "    def forward(self):\n",
        "        raise NotImplementedError   \n",
        "\n",
        "    def act(self, state):\n",
        "\n",
        "        action_probs = self.actor(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        #print(action_probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "        #print('sample result',action)\n",
        "\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        \n",
        "        return action.detach(), action_logprob.detach()    \n",
        "\n",
        "        '''\n",
        "        action_probs = self.actor(state)\n",
        "        action_probs_mode = action_probs[0:3] # 3 modes are considered\n",
        "        action_probs_route = action_probs[3:]\n",
        "\n",
        "        dist_mode = Categorical(action_probs_mode)\n",
        "        dist_route = Categorical(action_probs_route)\n",
        "        #print(action_probs)\n",
        "\n",
        "        action_mode = dist_mode.sample()\n",
        "        action_route = dist_route.sample()\n",
        "        action=torch.cat((action_mode, action_route))\n",
        "        #print('sample result',action)\n",
        "\n",
        "        #action_logprob=dist.log_prob(action)\n",
        "        action_logprob=torch.log(dist_mode[action_mode]*dist_route[action_route])\n",
        "         '''        \n",
        "        return action.detach(), action_logprob.detach()      \n",
        "   \n",
        "    def evaluate(self, state, action):\n",
        "\n",
        "        state = state.view(-1, 1)\n",
        "\n",
        "        action_probs = self.actor(state)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "        \n",
        "        return action_logprobs, state_values, dist_entropy\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std_init=0.6):\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "        \n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.AdamW([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        \n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "          with torch.no_grad():\n",
        "              state = torch.FloatTensor(state).to(device)\n",
        "              action, action_logprob = self.policy_old.act(state)\n",
        "            \n",
        "          self.buffer.states.append(state)\n",
        "          self.buffer.actions.append(action)\n",
        "          self.buffer.logprobs.append(action_logprob)\n",
        "\n",
        "          return action.item()\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # Monte Carlo estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "            \n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "\n",
        "        \n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            #logprobs, state_values, dist_entropy =  np.vectorize(self.policy.evaluate)(old_states, old_actions)\n",
        "\n",
        "            # Evaluating old actions and values    \n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "            \n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss\n",
        "            advantages = rewards - state_values.detach()   \n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # final loss of clipped objective PPO\n",
        "            #loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy #0.5, 0.01\n",
        "            loss = -torch.min(surr1, surr2)+ 0.5*self.MseLoss(state_values, rewards)- 0.01*dist_entropy\n",
        "            \n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # clear buffer\n",
        "        self.buffer.clear()\n",
        "    \n",
        "    \n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "   \n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "################################### Training ###################################\n",
        "\n",
        "\n",
        "####### initialize environment hyperparameters ######\n",
        "\n",
        "env_name = \"short path\"\n",
        "has_continuous_action_space = False\n",
        "\n",
        "max_ep_len = 400                  # max timesteps in one episode 400\n",
        "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
        "\n",
        "action_std = None\n",
        "\n",
        "################ PPO hyperparameters ################\n",
        "\n",
        "update_timestep = max_ep_len* 4       # update policy every n timesteps* 4\n",
        "K_epochs = 40               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO, 0.2\n",
        "gamma = 0.99                # discount factor 0.99\n",
        "\n",
        "lr_actor = 0.0003     # learning rate for actor network 0.0003\n",
        "lr_critic = 0.001     # learning rate for critic network 0.001 1 performs good\n",
        "#lr_actor = 0.0001;lr_critic = 0.005\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "print(\"training environment name : \" + env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = 1\n",
        "\n",
        "# action space dimension\n",
        "action_dim = 18\n",
        "\n",
        "###################### logging ######################\n",
        "\n",
        "#### log files for multiple runs are NOT overwritten\n",
        "\n",
        "log_dir = \"PPO_logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "log_dir = log_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "#### get number of log files in log directory\n",
        "run_num = 0\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "run_num = len(current_num_files)\n",
        "\n",
        "\n",
        "#### create new log file for each run \n",
        "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "\n",
        "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
        "print(\"logging at : \" + log_f_name)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "################### checkpointing ###################\n",
        "\n",
        "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "directory = \"PPO_preTrained\"\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"save checkpoint path : \" + checkpoint_path)\n",
        "\n",
        "def state_determin(action):\n",
        "  new_state=np.array([action])\n",
        "\n",
        "  return new_state\n",
        "\n",
        "def reward_determine(old_state,state):\n",
        "  \n",
        "  end_bonus=0\n",
        "  old_terminal= old_state[0]%6\n",
        "  old_mode= int(old_state[0]/6)\n",
        "  terminal= state[0]%6\n",
        "  mode= int(state[0]/6)\n",
        "\n",
        "  time_matrix=np.zeros([3,6,6])\n",
        "  time_matrix[0,0,2] = 1\n",
        "  time_matrix[0,0,1] = 2\n",
        "  time_matrix[1,2,4] = 1\n",
        "  time_matrix[1,1,3] = 2\n",
        "  time_matrix[2,4,5] = 1\n",
        "  time_matrix[2,3,5] = 2\n",
        "\n",
        "  i=mode\n",
        "  j=old_terminal\n",
        "  k=terminal\n",
        "\n",
        "  if time_matrix[i,j,k]==0:\n",
        "    penalty = 100\n",
        "  else:\n",
        "    penalty = 0\n",
        "  if state[0]%6 == 5 and old_state[0]!= 0:\n",
        "      end_bonus=1000\n",
        "\n",
        "  reward=-time_matrix[i,j,k]-penalty+end_bonus\n",
        "  return reward\n",
        "\n",
        "'''\n",
        "def travel_time_determine(old_state,state):\n",
        "    time_matrix = np.array([[0,1,2,0,0,0],\n",
        "                      [2,0,0,1,2,0],\n",
        "                      [2,0,0,2,2,0],\n",
        "                      [0,2,2,0,0,1],\n",
        "                      [0,2,2,0,0,2],\n",
        "                      [0,0,0,2,2,0]])\n",
        "    i=old_state[0]\n",
        "    j=state[0]\n",
        "    travel_time = time_matrix[i,j]\n",
        "    return travel_time\n",
        "'''\n",
        "\n",
        "def done_determine(state):\n",
        "  if state[0]%6 ==5 :\n",
        "    done=1\n",
        "  else:\n",
        "    done=0\n",
        "  return done\n",
        "\n",
        "################# training procedure ################\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std)\n",
        "\n",
        "# track total training time\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "# logging file\n",
        "log_f = open(log_f_name,\"w+\")\n",
        "log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "# printing and logging variables\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "\n",
        "log_running_reward = 0\n",
        "log_running_episodes = 0\n",
        "\n",
        "time_step = 0\n",
        "i_episode = 0\n",
        "\n",
        "#state_sample=[0]\n",
        "\n",
        "# training loop\n",
        "while time_step <= max_training_timesteps:\n",
        "\n",
        "    #print('new round')\n",
        "    state = np.array([0]) #起点  terminal 0\n",
        "    current_ep_reward = 0\n",
        "    #arrival_t=0\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        # exmaine the activation of states\n",
        "        #if  t >= arrival_t:\n",
        "          # select action with policy\n",
        "          #print('before',state)\n",
        "\n",
        "        \n",
        "\n",
        "          action = ppo_agent.select_action(state)\n",
        "          previous_state=state\n",
        "          state = state_determin(action)\n",
        "          #print('after',state)\n",
        "          \n",
        "          \n",
        "\n",
        "          reward = reward_determine(previous_state,state)#It is a floating data type value.\n",
        "          is_over=done_determine(state)          #if 到达终点 done=1\n",
        "          #route_time=travel_time_determine(previous_state,state)\n",
        "          #arrival_t = t + route_time\n",
        "\n",
        "          #state_sample.append(reward)# added\n",
        "          '''\n",
        "          # update activation\n",
        "          if t >= arrival_t:\n",
        "              activation = 1\n",
        "          else:\n",
        "              activation = 0\n",
        "           '''\n",
        "          # saving reward and is_terminals\n",
        "          ppo_agent.buffer.rewards.append(reward)\n",
        "          ppo_agent.buffer.is_terminals.append(is_over)\n",
        "          \n",
        "          time_step +=1\n",
        "          current_ep_reward += reward\n",
        "          \n",
        "          # update PPO agent\n",
        "          if time_step % update_timestep == 0:\n",
        "              ppo_agent.update()\n",
        "              \n",
        "          # log in logging file\n",
        "          if time_step % log_freq == 0:\n",
        "\n",
        "              # log average reward till last episode\n",
        "              log_avg_reward = log_running_reward / log_running_episodes\n",
        "              log_avg_reward = np.round(log_avg_reward, 4)\n",
        "\n",
        "              log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
        "              log_f.flush()\n",
        "\n",
        "              log_running_reward = 0\n",
        "              log_running_episodes = 0\n",
        "\n",
        "          # printing average reward\n",
        "          if time_step % print_freq == 0:\n",
        "              \n",
        "\n",
        "              # print average reward till last episode\n",
        "              print_avg_reward = print_running_reward / print_running_episodes\n",
        "              print_avg_reward = np.round(print_avg_reward, 2)\n",
        "\n",
        "              print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "\n",
        "              print_running_reward = 0\n",
        "              print_running_episodes = 0\n",
        "\n",
        "              #print(state_sample) #added\n",
        "              #state_sample = [0]\n",
        "                     \n",
        "          # save model weights\n",
        "          if time_step % save_model_freq == 0:\n",
        "              print(\"--------------------------------------------------------------------------------------------\")\n",
        "              print(\"saving model at : \" + checkpoint_path)\n",
        "              ppo_agent.save(checkpoint_path)\n",
        "              print(\"model saved\")\n",
        "              print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
        "              print(\"--------------------------------------------------------------------------------------------\")\n",
        "            \n",
        "          # break; if the episode is over\n",
        "          if is_over==1:\n",
        "              break\n",
        "\n",
        "\n",
        "    print_running_reward += current_ep_reward\n",
        "    print_running_episodes += 1\n",
        "\n",
        "    log_running_reward += current_ep_reward\n",
        "    log_running_episodes += 1\n",
        "\n",
        "    i_episode += 1\n",
        "\n",
        "\n",
        "log_f.close()\n",
        "\n",
        "\n",
        "# print total training time\n",
        "print(\"============================================================================================\")\n",
        "end_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "print(\"Finished training at (GMT) : \", end_time)\n",
        "print(\"Total training time  : \", end_time - start_time)\n",
        "print(\"============================================================================================\")\n"
      ]
    }
  ]
}